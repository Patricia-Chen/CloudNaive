

| 姓名           | 学号      |
| -------------- | --------- |
| 丁思璋（组长） | 211250023    |
| 陈泓雨         | 211250012 |
| 王骏佳         | 211250206 |

项目地址：https://gitee.com/wangchunchia/CloudNaive.git

## 作业说明

开发一个 Spring Boot 应用，并使用云原生功能

## 1.功能要求

### 1.1 实现一个 REST 接口， json 串  {"msg"：“hello”}

- #### 运行截图：

![](https://hackmd.io/_uploads/HygfGDD2h.png)


- #### 代码截图：

  service：

 ![](https://hackmd.io/_uploads/SJ-wgzRjh.png)


  domain：

  ![](https://hackmd.io/_uploads/rJy_lGAo3.png)


  controller（其中包含了限流的实现代码）：

  ![](https://hackmd.io/_uploads/S1LjefAj3.png)




### 1.2 接口提供限流功能，当请求达到每秒 100 次的时候，返回 429

- #### 代码实现：

  限流代码：

  ![](https://hackmd.io/_uploads/HkTjlMAo3.png)


  抛出异常代码：

  ![](https://hackmd.io/_uploads/Syi3gMCi3.png)


  

  具体思路为使用Guava实现限流器。

  Google的Guava工具包中提供了一个限流工具类——RateLimiter。RateLimiter是基于“令牌桶算法”来实现限流的。

  RateLimiter的原理类似于令牌桶，它主要由许可发出的速率来定义，如果没有额外的配置，许可证将按每秒许可证规定的固定速度分配，许可将被平滑地分发，若请求超过permitsPerSecond则RateLimiter按照每秒 1/permitsPerSecond 的速率释放许可。

  > #### **令牌桶算法**
  >
  > 令牌桶算法是一个存放固定容量令牌（token）的桶，按照固定速率往桶里添加令牌。令牌桶算法基本可以用下面的几个概念来描述：
  >
  > 1. 假如用户配置的平均发送速率为r，则每隔1/r秒一个令牌被加入到桶中。
  > 2. 桶中最多存放b个令牌，当桶满时，新添加的令牌被丢弃或拒绝。
  > 3. 当一个n个字节大小的数据包到达，将从桶中删除n个令牌，接着数据包被发送到网络上。
  > 4. 如果桶中的令牌不足n个，则不会删除令牌，且该数据包将被限流（要么丢弃，要么缓冲区等待）。

​		由于要求为“达到”每秒一百次，则认为一秒钟最多能通过99次请求，在第100次请求时，返回429。

- #### 运用JMeter进行测试：

  **首先，测试线程数为100时，是否仅有一个线程被限流，返回429。**

  建立线程，定义一个线程数为100、ramp-up时间为1秒、循环次数为1的线程组：

  ![](https://hackmd.io/_uploads/HJbRgG0jh.png)


  HTTP Request，向http://localhost:8080/发送请求：

  ![](https://hackmd.io/_uploads/B1rkZMRsh.png)


  计时器：

  ![](https://hackmd.io/_uploads/HJ1l-fAsn.png)


  进行压力测试，发现达到每秒100次后返回429（Too Many Requests）：

  ![](https://hackmd.io/_uploads/H1ogZz0j2.png)


  正常返回：

  ![](https://hackmd.io/_uploads/H1bb-fAi3.png)


  控制台输出：

  ![](https://hackmd.io/_uploads/S1FZWMAjh.png)


​	**然后，测试线程数为110时，是否有十一个线程被限流，返回429。**

建立线程，定义一个线程数为110、ramp-up时间为1秒、循环次数为1的线程组：

![](https://hackmd.io/_uploads/BJsfWGRih.png)


测试后，共有十一个线程返回429，符合预期：

![](https://hackmd.io/_uploads/rk7QZMCj2.png)

![](https://hackmd.io/_uploads/HkcQWzCj3.png)

### 1.3 加分项：当后端服务有多个实例的时候（一个 Service 包含若干个 Pod），如何实现统一限流
实现思路：使用admin来代理服务，通过admin控制所有访问service的服务，从而实现当后端服务有多个实例的时候（一个 Service 包含若干个 Pod），统一限流。

项目结构如下：
![](https://hackmd.io/_uploads/ryh3Xvvhh.png)

- #### 代码实现：
```java
public @interface RequestLimit {
    int count() default Integer.MAX_VALUE;
    long time() default 1000;
}
```
算法为利用Map记录每个接口的访问次数，每次访问均与设置的限制次数（即上面的count）比较，实现限流：
```java
   @Around("RequestLimit()")
    public synchronized Object requestLimit(ProceedingJoinPoint joinPoint) throws Throwable {

        HttpServletRequest request = ((ServletRequestAttributes) RequestContextHolder.getRequestAttributes()).getRequest();

        String url = request.getRequestURI();
        RequestLimit rateLimiter = getRequestLimit(joinPoint);

        String key = "req_limit_".concat(url); 
        if (!redisTemplate.containsKey(key)) { 
            redisTemplate.put(key, 1);
        } else {
            redisTemplate.put(key, redisTemplate.get(key) + 1);
            int count = redisTemplate.get(key);
            if (count > rateLimiter.count()) {
                throw new RequestLimitException();
            }else {
                Timer timer = new Timer();
                TimerTask task = new TimerTask() {   
                    @Override
                    public synchronized void run() {
                        redisTemplate.remove(key);
                    }
                };
                timer.schedule(task, rateLimiter.time());
            }
        }
        return joinPoint.proceed();
    }
```
返回429（TOO_MANY_REQUESTS）异常(此处设置的限流数为20）：
```java
@GetMapping("/greeting")
	@RequestLimit(count = 20)
	public Object greeting() {
		return greetingService.greeting();
	}
```
```java
@ResponseStatus(value = HttpStatus.TOO_MANY_REQUESTS)
public class RequestLimitException extends Exception{
}
```
- #### 测试：
通过admin调用接口，证明服务拆分成功：
![](https://hackmd.io/_uploads/BJ1bavP3h.png)

运用JMeter进行测试，@RequestLimit(count = 20)，线程数为35：
![](https://hackmd.io/_uploads/HyzItwD22.png)
![](https://hackmd.io/_uploads/ryghYvP3h.png)

![](https://hackmd.io/_uploads/rk7QZMCj2.png)

- 查看输出
greeting-service1处理了6次请求：
![](https://hackmd.io/_uploads/Sysb3vDnh.png)
greeting-service2处理了8次请求：![](https://hackmd.io/_uploads/H1munDwn3.png)
greeting-service3处理了6次请求：![](https://hackmd.io/_uploads/r1wKhvP22.png)

6+8+6=20，而@RequestLimit(count = 20)，所以验证成功。


## 2. DevOps 要求

### 2.1 Dockerfile，用于构建镜像

```dockerfile
# Dockerfile
  
FROM java:8

RUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
RUN echo 'Asia/Shanghai' >/etc/timezone

WORKDIR /app
ADD target/CloudNative-0.0.1-SNAPSHOT.jar .

ENTRYPOINT ["java","-jar","CloudNative-0.0.1-SNAPSHOT.jar"]

```

### 2.2 Kubernetes 编排文件

```yaml
#cloud-native.yaml

apiVersion: apps/v1
kind: Deployment #对象类型
metadata:
  labels:
    app: cloud-native
  name: cloud-native
  namespace: nju25
spec:
  replicas: 1  #运行容器的副本数
  selector:
    matchLabels:
      app: cloud-native
  template:
    metadata:
      annotations:
        prometheus.io/path: /actuator/prometheus
        prometheus.io/port: "8080"
        prometheus.io/scheme: http
        prometheus.io/scrape: "true"
      labels:
        app: cloud-native
    spec:
      containers: #docker容器的配置
        - image: harbor.edu.cn/nju25/cloud-native:{VERSION} #pull镜像的地址
          name: cloud-native
      imagePullSecrets:
        - name: nju25-secret
---
apiVersion: v1
kind: Service
metadata:
  name: cloud-native
  namespace: nju25
  labels:
    app: cloud-native
spec:
  type: NodePort
  selector:
    app: cloud-native
  ports:
    - name: tcp
      nodePort: 31105  #host's port
      protocol: TCP
      port: 8080  #service's port
      targetPort: 8080  #target pod's port

```

**Secret 文件获取harbor访问权限**

```yaml
#secret.yaml

apiVersion: v1
data:
  .dockerconfigjson: ewoJImF1dGhzIjogewoJCSJoYXJib3IuZWR1LmNuIjogewoJCQkiYXV0aCI6ICJibXAxTWpVNmJtcDFNalV5TURJeiIKCQl9Cgl9Cn0=
kind: Secret
metadata:
  name: nju25-secret
type: kubernetes.io/dockerconfigjson
```

### 2.3 持续集成、持续部署流水线

合并了持续集成和部署流水线，各阶段如下：

|          | 阶段                    | 内容                      |
| -------- | ----------------------- | ------------------------- |
| 持续集成 | Git Clone Stage         | 克隆代码，为了build image |
|          | Maven Build Stage       | maven打包                 |
|          | Image Build Stage       | 构建镜像                  |
|          | Push Docker Image Stage | 推送至Harbor镜像仓库      |
| 持续部署 | Git clone YAML To Slave | 拉取代码，为了部署到k8s   |
|          | Change YAML File Stage  | 替换YAML文件变量          |
|          | Deploy To K8s Stage     | 部署到k8s                 |

```Groovy
//Jenkinsfile
pipeline {
    agent none
    stages {
        stage('Clone Code') {
            agent {
                label 'master'
            }
            steps {
                echo "1.Git Clone Code"
                git branch: 'main', url: "https://gitee.com/wangchunchia/nju25-cloud-native.git"
            }
        }
        stage('Maven Build') {
            agent {
                docker {
                    image 'maven:latest'
                    args '-v /root/.m2:/root/.m2'
                }
            }
            steps {
                echo "2.Maven Build Stage"
                sh 'mvn -B clean package -Dmaven.test.skip=true'
            }
        }
        stage('Image Build') {
            agent {
                label 'master'
            }
            steps {
            echo "3.Image Build Stage"
            sh 'docker build -f Dockerfile --build-arg jar_name=target/cloud-native-0.0.1-SNAPSHOT.jar -t cloud-native:${BUILD_ID} . '
            sh 'docker tag  cloud-native:${BUILD_ID}  harbor.edu.cn/nju25/cloud-native:${BUILD_ID}'
            }
        }
        stage('Push') {
            agent {
                label 'master'
            }
            steps {
            echo "4.Push Docker Image Stage"
            sh "docker login --username=nju25 harbor.edu.cn -p nju252023"
            sh "docker push harbor.edu.cn/nju25/cloud-native:${BUILD_ID}"
            }
        }
    }
}


node('slave') {
    container('jnlp-kubectl') {
        
        stage('Clone YAML') {
        echo "5. Git Clone YAML To Slave"
        git url: "https://gitee.com/wangchunchia/nju25-cloud-native.git", branch: 'main'
        }
        
        stage('YAML') {
        echo "6. Change YAML File Stage"
        sh 'sed -i "s#{VERSION}#${BUILD_ID}#g" ./jenkins/scripts/cloud-native.yaml'
        }
    
        stage('Deploy') {
        echo "7. Deploy To K8s Stage"
        sh 'kubectl apply -f ./jenkins/scripts/cloud-native.yaml -n nju25'
        }
    }
}

```

编辑secret.yaml如下：

```yaml
apiVersion: v1
data:
  .dockerconfigjson: ewoJImF1dGhzIjogewoJCSJoYXJib3IuZWR1LmNuIjogewoJCQkiYXV0aCI6ICJibXAxTWpVNmJtcDFNalV5TURJeiIKCQl9Cgl9Cn0=
kind: Secret
metadata:
  name: nju25-secret
type: kubernetes.io/dockerconfigjson
```

配置secret
![](https://hackmd.io/_uploads/Hkk8SKAsn.png)



流水线部署成功：

![](https://hackmd.io/_uploads/HJJV4kRs2.png)

镜像仓库：
![](https://hackmd.io/_uploads/Sk2sByCo2.png)



部署产物：

![](https://hackmd.io/_uploads/SJJKEkCj2.png)

浏览器访问验证：

![](https://hackmd.io/_uploads/HJFINy0oh.png)

## 3.扩容场景

### 3.1 Prometheus 采集监控指标

在springboot项目中配置Prometheus metrics 接口


```properties
#application.properties
server.port=8080
management.endpoints.web.base-path=/actuator
management.server.port=8080
management.endpoints.web.exposure.include=prometheus
management.prometheus.metrics.export.enabled=true
management.endpoint.health.show-details=always
management.metrics.tags.application=${spring.application.name}
```


声明一个ServiceMonitor对象，在流水线中的Deploy阶段部署

```yaml
#cloud-native-serviceMonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    k8s-app: cloud-native
  name: cloud-native
  namespace: monitoring
spec:
  endpoints:
  - interval: 30s
    port: tcp
    path: /actuator/prometheus
    scheme: 'http'
  selector:
    matchLabels:
      app: cloud-native
  namespaceSelector:
    matchNames:
    - nju25
```

声明一个ServiceMonitor对象，在流水线中的Deploy阶段部署


在Prometheus的UI界面验证

![](https://hackmd.io/_uploads/H1s5MGAih.png)

![](https://hackmd.io/_uploads/rk8FzzRs3.png)


### 3.2Grafana 定制应用监控大屏

![](https://hackmd.io/_uploads/B1NWPvCj3.png)
1. container cpu usage 容器CPU使用情况

![](https://hackmd.io/_uploads/HJAAwv0i2.png)

2. packets received 接收包数

![](https://hackmd.io/_uploads/SyyM_w0s2.png)

3. packets transmit rate 包传输率

![](https://hackmd.io/_uploads/Bkh4dw0j2.png)

4. total http server requests http请求总数

![](https://hackmd.io/_uploads/BJ2vOwCjh.png)

5. packets transmitted 传输包数

![](https://hackmd.io/_uploads/ByaFdvRs3.png)

6. container cpu usage 容器内存使用情况

![](https://hackmd.io/_uploads/BJ3puwRo2.png)

### 3.3 压测并观察监控数据

定义一个线程数为100、ramp-up时间为1秒、循环次数为1的线程组并向http://172.29.4.18:31105/limit/hello发送请求:

![](https://hackmd.io/_uploads/rkG-w_J32.png)
![](https://hackmd.io/_uploads/Sy5HwOknh.png)


压测后的监控数据:

![](https://hackmd.io/_uploads/SyXPwuynh.png)

可以看到Memory、传输包数等均有明显变化。

### 3.4 手工扩容并观察监控数据

扩容前pod数为1:

![](https://hackmd.io/_uploads/B1NMd_yhh.png)

更改yaml文件重新构建流水线：

![](https://hackmd.io/_uploads/BklXu_13h.png)

扩容成功：

![](https://hackmd.io/_uploads/HkDj_OJ22.png)


扩容后的监控数据:

![](https://hackmd.io/_uploads/HkzSOuJn3.png)

可以看到Cpu、Memory、packets transmit rate、http server request等的监控面板新增两个Container的曲线。